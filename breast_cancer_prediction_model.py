# -*- coding: utf-8 -*-
"""Breast_Cancer_Prediction_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NacqRuvzVwLHxykp5AAr6IiieTtJysHX

#**Machine Learning in Medical Industry - Breast Cancer Study Case**

Breast cancer is cancer that develops from breast tissue. Signs of breast cancer may include a lump in the breast, a change in breast shape, dimpling of the skin, milk rejection, fluid coming from the nipple, a newly inverted nipple, or a red or scaly patch of skin. In those with distant spread of the disease, there may be bone pain, swollen lymph nodes, shortness of breath, or yellow skin. Outcomes for breast cancer vary depending on the cancer type, the extent of disease, and the person's age. Based on data from GLOBOCAN 2022, breast cancer is the number 1 cancer suffered by Indonesian women. A total of 66,271 new cases of breast cancer occurred in Indonesia in 2022. In addition, breast cancer has the highest case in Indonesia when calculated from the overall gender cancer case.

###**About the dataset**
The dataset is taken from UCI Machine Learning Repository.
There are 3 things to achieve in this code:

1.   In-Depth Data Analysis

2.   Predict Breast Cancer Risk based on clinical features
3.   Machine Learning Model Comparison with Gradient Boosting, Random Forest, and Stacking

###**Attribute information:**

* (1) ID number

* (2) Diagnosis (M = malignant, B = benign)

* (3-32) Ten real-valued features are computed for each cell nucleus:

  * radius (mean of distances from center to points on the perimeter)

  * texture (standard deviation of gray-scale values)
  * perimeter
  * area
  * smoothness (local variation in radius lengths)
  * compactness (perimeter^2 / area - 1.0)
  * concavity (severity of concave portions of the contour)
  * concave points (number of concave portions of the contour)
  * symmetry
  * fractal dimension ("coastline approximation" - 1)

The mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.

### Import Library

Pada tahap ini, dilakukan import library yang dibutuhkan selama pengerjaan project.
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix

import warnings
warnings.filterwarnings('ignore')

"""### Load Dataset

Load dataset breast cancer dari GitHub (data sama dengan yang di UCI Machine Learning)
"""

url = 'https://raw.githubusercontent.com/pkmklong/Breast-Cancer-Wisconsin-Diagnostic-DataSet/master/data.csv'
df = pd.read_csv(url)

"""### Sneak Peak Data"""

#Looking at the first 5 rows of the dataset
df.head()

#Looking at the last 5 rows of the dataset
df.tail()

#How many rows and columns in the dataset?
df.shape

"""Terdapat 569 baris dan 33 kolom pada dataset."""

#Removing unused columns
df.drop('id', axis=1, inplace=True)
df.drop('Unnamed: 32', axis=1, inplace=True)

"""Data memiliki 1 kolom identitas (ID) dan 1 kolom 'unknown' yang tidak memiliki kegunaan, sehingga kedua kolom tersebut dihapus. Kolom identitas harus dihapus untuk mencegah terjadinya overfitting pada model yang dibuat, karena jika ada identitas pembeda, model akan belajar untuk menghafal identitas tersebut bukan melihat pola."""

#General information of the dataset
df.info()

"""### Handling Missing Values"""

#Checking for missing values
df.isnull().sum()

"""Pada dataset, tidak ada data yang kosong atau memiliki tipe data yang tidak sesuai sehingga tidak perlu dilakukan imputasi atau pengubahan tipe data.

## Exploratory Data Analysis

Informasi statistik pada kolom numerik di dataset dilihat dengan kode dibawah ini.
"""

#Describing the dataset
df.describe()

#Univariate analysis 'diagnosis'
data_plot  = df['diagnosis'].value_counts().to_list()
label_plot = df['diagnosis'].value_counts().index.to_list()

title = 'Breast Cancer Diagnosis'

plot       = sns.barplot(x = label_plot, y = data_plot, palette = 'CMRmap')
plot_title = plt.title(title)

plt.show()

df['diagnosis'].value_counts()

color = sns.color_palette('Dark2')

plot       = plt.pie(data_plot, labels=label_plot, colors=color, autopct='%.0f%%')
plot_title = plt.title(title)

plt.show()

"""Membuat chart untuk melihat persebaran data diagnosis. Pada dataset terdapat 357 pasien dengan status benign (tumor jinak), dan 212 pasien dengan status malignant (kanker). Dengan ini, 63% pasien di dataset memiliki status benign. Sedangkan 37% terjangkit kanker payudara (malignant)."""

#Boxplot of numeric variables
column_name_list_num = ['radius_mean', 'texture_mean', 'perimeter_mean',
       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
       'fractal_dimension_se', 'radius_worst', 'texture_worst',
       'perimeter_worst', 'area_worst', 'smoothness_worst',
       'compactness_worst', 'concavity_worst', 'concave points_worst',
       'symmetry_worst', 'fractal_dimension_worst']

#Create subplots
num_cols = len(column_name_list_num)
num_rows = (num_cols + 2) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15,5*num_rows))
axs = axs.flatten()

#Boxplot for each variables
for i, var in enumerate (column_name_list_num):
  sns.boxplot(x=var, data=df, palette = 'CMRmap', ax=axs[i])
  axs[i].set_title("Boxplot of" + " " + var)
  axs[i].tick_params(axis='x', rotation=90)

#Removes extra empty subplots
if num_cols < len(axs):
  for i in range(num_cols, len(axs)):
    fig.delaxes(axs[i])

fig.tight_layout()
plt.show()

"""Box plot adalah visualisasi yang dibuat untuk menggambarkan bentuk distribusi dan penyebaran data. Pada box plot dapat terlihat kuartil, jarak antar kuartil, batas minimum dan maksimum, serta outlier pada data. Dibuat box plot untuk masing-masing kolom numerik."""

#Create subplots
num_cols = len(column_name_list_num)
num_rows = (num_cols + 2) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15,5*num_rows))
axs = axs.flatten()

#Violin plot for each variables
for i, var in enumerate (column_name_list_num):
  sns.violinplot(x=var, data=df, palette = 'CMRmap', ax=axs[i])
  axs[i].set_title(var)
  axs[i].tick_params(axis='x', rotation=90)

#Removes extra empty subplots
if num_cols < len(axs):
  for i in range(num_cols, len(axs)):
    fig.delaxes(axs[i])

fig.tight_layout()
plt.show()

"""Violin plot adalah variasi dari box plot yang ditambah kernel density plot. Dibuat violin plot untuk masing-masing kolom numerik.

"""

#Histogram overlay of 'diagnosis' with the independent variables
#Create subplots
num_cols = len(column_name_list_num)
num_rows = (num_cols + 2) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15,5*num_rows))
axs = axs.flatten()

#Histplot for each variables
for i, var in enumerate (column_name_list_num):
  sns.histplot(x=var, hue = 'diagnosis', data=df, palette = 'CMRmap', ax=axs[i])
  axs[i].set_title(var + " " + "by target")
  axs[i].tick_params(axis='x', rotation=90)

#Removes extra empty subplots
if num_cols < len(axs):
  for i in range(num_cols, len(axs)):
    fig.delaxes(axs[i])

fig.tight_layout()
plt.show()

"""Histogram adalah grafik untuk melihat pola distribusi data. Dibuat histogram untuk masing-masing kolom numerik dengan overlay status diagnosis untuk melihat perbedaan penyebaran data numerik pada setiap jenis diagnosis.

Dapat dilihat bahwa pada dataset, mayoritas pasien dengan status benign memiliki hasil pengukuran tumor yang lebih kecil.
"""

#Create subplots
num_cols = len(column_name_list_num)
num_rows = (num_cols + 2) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15,5*num_rows))
axs = axs.flatten()

#Violin plot for each variables
for i, var in enumerate (column_name_list_num):
  sns.violinplot(y=var, x='diagnosis', data=df, palette = 'CMRmap', ax=axs[i])
  axs[i].set_title("Violin plot of" + " " + var + " " + "by Diagnosis")
  axs[i].tick_params(axis='x', rotation=90)

#Removes extra empty subplots
if num_cols < len(axs):
  for i in range(num_cols, len(axs)):
    fig.delaxes(axs[i])

fig.tight_layout()
plt.show()

"""Violin plot untuk masing-masing kolom numerik dengan overlay status diagnosis untuk melihat perbedaan distribusi data numerik pada setiap jenis diagnosis."""

#Mean of each column values in each class
df.groupby(by = 'diagnosis').mean()

#Correlations between features
matrix = df.corr().round(2)
plt.figure(figsize=(16,16))
sns.heatmap(matrix, annot=True, vmax=1, vmin=-1, center=0, cmap='coolwarm')
plt.title("Feature Correlation")
plt.show()

"""Korelasi antar fitur numerik dapat dilihat pada gambar diatas. Nilai korelasi dibulatkan menjadi 2 angka dibelakang nol. Semakin tinggi nilai maka korelasi semakin kuat. Korelasi bernilai positif berarti berbanding lurus, jika korelasi bernilai negatif maka korelasi berbanding terbalik.

##Labelling Categorical Data
"""

#Labeling categorical data
diagnosis = {
    "B": 0,
    "M": 1
}

df['diagnosis'] = df['diagnosis'].map(diagnosis)

"""Hasil diagnosis bersifat categorical sehingga harus diubah ke numerik agar dapat digunakan model. Dilakukan labelling value kolom diagnosis dengan "B" = 0 dan "M" = 1. Labelling dilakukan dengan mapping value berdasarkan dictionary yang telah dibuat.

## Balancing Data + Splitting the dataset into the Training set and Test set
"""

#Defining x and y
x = df.drop(columns=['diagnosis'])
y = df['diagnosis']

"""Setelah labelling data, data dibagi menjadi x dan y. X berisi variabel penentu hasil diagnosis (seluruh kolom numerik kecuali diagnosis) dan Y berisi variabel target (diagnosis).

"""

from imblearn.over_sampling import SMOTE
#define oversampling strategy
SMOTE = SMOTE()

#fit and apply the transform
x_SMOTE, y_SMOTE = SMOTE.fit_resample(x, y)

"""Selanjutnya, dilakukan balancing data dengan metode oversampling SMOTE karena jumlah data tidak terlalu seimbang. Balancing data dilakukan agar model dapat mencapai akurasi yang lebih tinggi.

"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_SMOTE, y_SMOTE, test_size = 0.2, random_state = 42)

"""Dataset lalu dibagi menjadi train dan test dengan rasio 80:20. Alasan kenapa ditetapkan jumlah tersebut adalah karena pada umumnya 80/20 dianggap cukup baik (kecuali jika data training sangatlah banyak, maka rasio data split bisa berubah). Pada dataset project ini, jumlah kolom tidak terlalu banyak sehingga diperlukan data training yang cukup untuk memastikan model terlatih dengan baik.

"""

x_train.shape, x_test.shape

"""## Modelling

### Gradient Boosting

Import algoritma dari sklearn.ensemble
"""

from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier()

"""Proses pelatihan/fitting model menggunakan x_train dan y_train yang telah dibagi sebelumnya."""

#Training the model
gb.fit(x_train, y_train)

"""Memprediksi data testing dengan model yang telah dilatih.

"""

#Predict testing set
y_pred_gb = gb.predict(x_test)

"""Mengevaluasi model dengan metrik-metrik yang telah ditentukan"""

print('Training-set accuracy score:', gb.score(x_train, y_train))
print('Test-set accuracy score:', gb.score(x_test, y_test))

plt.figure(figsize=(6,6))
sns.heatmap(confusion_matrix(y_test,y_pred_gb), annot=True, fmt='d', cmap='Purples')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Gradient Boosting')
plt.show()

#Check model performance using classification_report
print(classification_report(y_test, y_pred_gb))

#Specificity
cm = confusion_matrix(y_test, y_pred_gb)

specificity = cm[1,1]/(cm[1,0] + cm[1,1])
print('Specificity : ', specificity)

#Check model performance using auc score
roc_auc_score(y_test, y_pred_gb)*100

"""### Random Forest

Import algoritma Random Forest dari from sklearn.ensemble dan melatih model menggunakan x_train dan y_train
"""

from sklearn.ensemble import RandomForestClassifier

classifier_rf = RandomForestClassifier()
classifier_rf.fit(x_train, y_train)
y_pred_rf = classifier_rf.predict(x_test)

"""Mengevaluasi model dengan metrik-metrik yang telah ditentukan"""

print('Training-set accuracy score:', classifier_rf.score(x_train, y_train))
print('Test-set accuracy score:', classifier_rf.score(x_test, y_test))

plt.figure(figsize=(6,6))
sns.heatmap(confusion_matrix(y_test,y_pred_rf), annot=True, fmt='d', cmap='Purples')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Random Forest')
plt.show()

#Classification report
print(classification_report(y_test, y_pred_rf))

#Specificity
cm = confusion_matrix(y_test, y_pred_rf)

specificity = cm[1,1]/(cm[1,0] + cm[1,1])
print('Specificity : ', specificity)

roc_auc_score(y_test, y_pred_rf)*100

"""### Stacking Model

Import base model yang dibutuhkan seperti SVC, Decision Tree, Logistic Regression, Random Forest, XG Boost. Lalu, import metode stacking dengan kode "from sklearn.ensemble import StackingClassifier"
Setelah itu, gabungkan base model dan menentukan model final (meta-model) Pada project ini, final model adalah logistic regression. Pada gambar dibawah, dapat dilihat urutan model stacking.
Terakhir, lakukan pelatihan/fitting model menggunakan x_train dan y_train.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier

base_models = [('Decision Tree',DecisionTreeClassifier()),('Logistic Regression',LogisticRegression()), ('Random Forest', RandomForestClassifier()), ('xgb', XGBClassifier())]
stacking = StackingClassifier(estimators = base_models, final_estimator = LogisticRegression(), cv = 5)
stacking.fit(x_train , y_train)

""" Memprediksi data testing dengan model yang telah dilatih"""

y_pred_st = stacking.predict(x_test)

"""Mengevaluasi model dengan metrik-metrik yang telah ditentukan"""

print('Training-set accuracy score:', stacking.score(x_train, y_train))
print('Test-set accuracy score:', stacking.score(x_test, y_test))

plt.figure(figsize=(6,6))
sns.heatmap(confusion_matrix(y_test,y_pred_st), annot=True, fmt='d', cmap='Purples')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Stacking Classifier')
plt.show()

#Classification report
print(classification_report(y_test, y_pred_st))

#Specificity
cm = confusion_matrix(y_test, y_pred_st)

specificity = cm[1,1]/(cm[1,0] + cm[1,1])
print('Specificity : ', specificity)

roc_auc_score(y_test, y_pred_st)*100

"""#**Conclusion**
From the experiments and metrics comparison, it can be seen that the stacking model has higher scores in various metrics than the other two models (Gradient Boost and Random Forest).
Because this project is a project in healthcare, the model with the highest metrics will be sought to minimize the chance of error.

"""